"""
To enable a user to test multiple topic skills and a master workspace, according to REN (without exception handling).

To run you need:
- topic skill names and their skill IDs in Credentials.py
- a blindset for each skill in the data folder, named `{skill}_blindset.csv`. These will be concatenated to fire at master.
"""

# IMPORTS
# external
import pandas as pd
import numpy as np
from tqdm import tqdm
from ibm_watson import AssistantV1
import click
import os
import sys

sys.path.append("..")

# internal
import for_csv.logger
import logging

logger = logging.getLogger("multiskill")
logging.getLogger("matplotlib.font_manager").setLevel(logging.WARNING)

import conversation_test.blindset as blindset
from conversation_test.metrics import Metrics
from for_csv.utils import generate_timestamp, process_list_argument
import for_csv.WA_utils as wa_utils
import config
import Credentials

# get credentials
active_adoption = Credentials.active_adoption
instance_creds = Credentials.ctx[active_adoption]
conversation_version = Credentials.conversation_version
""" workspace_id = Credentials.workspace_id[active_adoption][topic]
workspace_thresh = Credentials.calculate_workspace_thresh(topic)
"""


@click.command()
@click.argument("topic_list")
@click.option(
    "--conf_matrix",
    "-c",
    is_flag=True,
    help="Whether to plot confusion matrices for all skills (including master).",
)
@click.option(
    "--save_master_data",
    "-s",
    is_flag=True,
    help="Whether to save the training set and blindset of the created master skill to the data folder.",
)
def main(topic_list, conf_matrix, save_master_data):
    skill_list = process_list_argument(topic_list, val_type=str)
    master_skill_id = None  # so exception works
    master_thresh = Credentials.calculate_workspace_thresh("master")

    try:
        id_dict = {skill: Credentials.workspace_id[active_adoption][skill] for skill in skill_list}
        timestr = generate_timestamp()  #  for use in all filenames

        # authenticate
        if "apikey" in instance_creds:
            logger.debug("Authenticating (apikey)")
            bs = blindset.blindset(
                apikey=instance_creds["apikey"],
                url=instance_creds["url"],
                version=conversation_version,
            )
        elif "password" in instance_creds:
            logger.debug("Authenticating (username/password)")
            bs = blindset.blindset(
                username=instance_creds["username"],
                password=instance_creds["password"],
                url=instance_creds["url"],
                version=conversation_version,
            )

        # check skills exist
        check_skills_exist(skill_list)

        #  import blindsets and generate master
        logger.info("Importing all blindsets and combining into master")
        blind_dict = dict()
        for skill in skill_list:
            bs_path = os.path.join(config.data_dir, f"{skill}_blindset.csv")
            blind_dict[skill] = bs.import_blindset(bs_path)

        master_blind_allcols = pd.concat(
            [v.assign(topic=k) for k, v in blind_dict.items()],
            axis=0,
            ignore_index=True,
            sort=False,
        )
        master_blind = master_blind_allcols[["utterance", "topic"]].rename(
            columns={"topic": "expected intent"}
        )

        # generate master from topic training and push to WA
        logger.info("Getting training data from WA")
        train_dict = dict()
        for skill in skill_list:
            train_dict[skill] = wa_utils.get_training_data(bs.assistant, id_dict[skill])

        logger.info("Creating temporary master skill")
        master_train = pd.concat(
            [v.drop(columns=["intent"]).assign(intent=k) for k, v in train_dict.items()],
            axis=0,
            ignore_index=True,
            sort=False,
        )
        master_skill_id = wa_utils.create_workspace_from_df(
            bs.assistant,
            name="master",
            train_df=master_train,
            description="generated by intent_training_tools",
        )

        # run blindset on master
        logger.info("Running blindset on master..")
        results_master = bs.run_blind_test(master_blind, master_skill_id, threshold=master_thresh)

        # create blindsets for topics based on master results
        res_anythingelse = results_master[results_master["confidence1"] < master_thresh]

        newblind_dict = dict()
        for skill in skill_list:
            # blindset for each skill is made up of utterances that have landed in that skill for master
            blind_utterances = results_master.loc[
                (results_master["intent1"] == skill)
                & (results_master["confidence1"] >= master_thresh),
                "original_text",
            ].tolist()
            newblind = master_blind_allcols[
                master_blind_allcols["utterance"].isin(blind_utterances)
            ].copy()
            newblind.loc[newblind["topic"] != skill, "expected intent"] = "anything_else"
            newblind_dict[skill] = newblind[["utterance", "expected intent"]].reset_index(drop=True)

        # run blindsets on topics
        logger.info("Running blindset on topic skills..")
        results_dict = dict()
        for skill in skill_list:
            results_dict[skill] = bs.run_blind_test(
                newblind_dict[skill],
                id_dict[skill],
                threshold=Credentials.calculate_workspace_thresh(skill),
            )

        #  plot confusion matrices
        if conf_matrix:
            from conversation_test.confusionmatrix import ConfusionMatrix

            conf_output_path = lambda s: os.path.join(
                config.output_folder, f"{s}_multi_confmat_{timestr}.png"
            )

            # master
            cfn = ConfusionMatrix(workspace_thresh=master_thresh)
            cfn.create(results_master, fig_path=conf_output_path("master"))

            #  topics
            for skill in skill_list:
                cfn = ConfusionMatrix(
                    workspace_thresh=Credentials.calculate_workspace_thresh(skill)
                )
                cfn.create(results_dict[skill], fig_path=conf_output_path(skill))

            logger.info("Confusion matrix saved to results folder")

        # calculate metrics
        # master
        met = Metrics(workspace_thresh=master_thresh)
        metrics_master, _ = met.get_all_metrics(results_master, detailed_results=True)

        # topics
        metrics_dict = dict()
        for skill in skill_list:
            met = Metrics(workspace_thresh=Credentials.calculate_workspace_thresh(skill))
            metrics_dict[skill], _ = met.get_all_metrics(results_dict[skill], detailed_results=True)

        # export results
        for skill in skill_list:
            results_dict[skill].to_csv(
                os.path.join(config.output_folder, f"{skill}_multi_results_{timestr}.csv"),
                index=None,
            )
            metrics_dict[skill].to_csv(
                os.path.join(config.output_folder, f"{skill}_multi_metrics_{timestr}.csv")
            )

        results_master.to_csv(
            os.path.join(config.output_folder, f"master_multi_results_{timestr}.csv"), index=None,
        )
        metrics_master.to_csv(
            os.path.join(config.output_folder, f"master_multi_metrics_{timestr}.csv")
        )
        logger.info("Results and metrics saved to output folder")

        if save_master_data:
            # export master blindset with both intent and topic labels to CSV
            master_blind_allcols.to_csv(
                os.path.join(config.data_dir, f"master_blindset_{timestr}.csv"), index=None,
            )

            # export master training to CSV
            master_train.to_csv(
                os.path.join(config.data_dir, f"master_training_{timestr}.csv"),
                header=None,
                index=None,
            )

            logger.info("Master blindset and training have also been saved to the data folder")

        #  delete master skill
        logger.info("Deleting temporary master skill")
        wa_utils.delete_workspace(bs.assistant, master_skill_id)

    except Exception as e:
        if master_skill_id is not None:
            # make sure master deleted anyway
            logger.info("Deleting temporary master skill before exit")
            wa_utils.delete_workspace(bs.assistant, master_skill_id)

        raise e


def check_skills_exist(skill_list):
    """
    Check skills are in both the Credentials file and that blindsets exist for them.
    """

    bs_notexist = []
    cr_notexist = []

    topics_in_creds = Credentials.workspace_id[active_adoption].keys()
    get_bs_path = lambda s: os.path.join(config.data_dir, f"{s}_blindset.csv")

    for skill in skill_list:
        # append skill to bs_notexist if blindset doesn't exist
        bs_path = get_bs_path(skill)
        if not os.path.exists(bs_path):
            bs_notexist.append(skill)

        #  append skill to cr_notexist if credential doesn't exist
        if skill not in topics_in_creds:
            cr_notexist.append(skill)

    if len(bs_notexist) > 0 & len(cr_notexist) > 0:
        raise ValueError(
            "Blindsets don't exist for topics {}, and Credentials don't exist for topics {}. Please add these and try again".format(
                bs_notexist, cr_notexist
            )
        )
    elif len(bs_notexist) > 0:
        raise ValueError(
            "Blindsets don't exist for topics {}. Please add these and try again".format(
                bs_notexist
            )
        )
    elif len(cr_notexist) > 0:
        raise ValueError(
            "Credentials don't exist for topics {}. Please add these and try again".format(
                cr_notexist
            )
        )
    else:
        logger.debug("All skills exist in credentials and blindsets")


if __name__ == "__main__":
    main()
